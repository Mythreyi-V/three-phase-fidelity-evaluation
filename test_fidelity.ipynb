{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install lime\n",
    "#!pip install shap\n",
    "#!pip install anchor-exp\n",
    "#!pip install hyperopt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from hyperopt import hp\n",
    "import pickle\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-deep')\n",
    "\n",
    "import statistics\n",
    "import scipy as scp\n",
    "import math\n",
    "\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "import shap\n",
    "\n",
    "from anchor import anchor_tabular\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mape(p1, diffs):\n",
    "    iters = len(diffs)\n",
    "    total_diffs = sum([abs(diff/p1) for diff in diffs])\n",
    "    mape = total_diffs/iters\n",
    "    \n",
    "    return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_excluding(exc_min, exc_max, min_val, max_val):\n",
    "    new_val = random.uniform(min_val, max_val)\n",
    "    \n",
    "    while new_val > exc_min and new_val < exc_max:\n",
    "        new_val = random.uniform(min_val, max_val)\n",
    "    return new_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_breakdown(exp_list, feat_locs, feat_names, train_data):\n",
    "    \n",
    "    distribs = []\n",
    "    \n",
    "    for i in range(len(feat_locs)):\n",
    "        feat_name = feat_names[i]\n",
    "                \n",
    "        min_vals = []\n",
    "        max_vals = []\n",
    "\n",
    "        for rule in exp_list:\n",
    "            if feat_name in rule:\n",
    "                loc = feat_locs[i]\n",
    "    \n",
    "                if rule.count(\"<\") == 1:\n",
    "                    min_vals.append(min([inst[loc] for inst in train_data]))\n",
    "                    max_vals.append(float(rule.split(\"<\")[-1].strip(\" \").strip(\"=\")))\n",
    "                    \n",
    "                elif rule.count(\"<\") == 0 and rule.count(\">\") == 1:\n",
    "                    min_vals.append(float(rule.split(\">\")[-1].strip(\" \").strip(\"=\")))\n",
    "                    max_vals.append(max([inst[loc] for inst in train_data]))\n",
    "                    \n",
    "                elif rule.count(\"<\") > 1:\n",
    "                    min_vals.append(float(rule.split(\"<\")[0].strip(\" \").strip(\"=\")))\n",
    "                    max_vals.append(float(rule.split(\"<\")[-1].strip(\" \").strip(\"=\")))\n",
    "        \n",
    "        distribs.append((feat_locs[i], min(min_vals), max(max_vals)))\n",
    "                \n",
    "    return distribs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lime_distribution(instance, explainer, cls, quartiles, train_data, classification):\n",
    "    lime_exp = []\n",
    "    for i in range(exp_iter):\n",
    "        if classification==True:\n",
    "            lime_exp.extend(explainer.explain_instance(instance, cls.predict_proba, \n",
    "                                                num_features=len(feat_list), labels=[0,1]).as_list())\n",
    "        else:\n",
    "            lime_exp.extend(explainer.explain_instance(instance, cls.predict, \n",
    "                                                    num_features=len(feat_list), labels=[0,1]).as_list())\n",
    "            \n",
    "    weights = [[] for each in feat_list]\n",
    "    for exp in lime_exp:\n",
    "        feat = exp[0].replace(\"= \",'')\n",
    "        if '<' in feat:\n",
    "            parts = feat.split('<')\n",
    "        elif '>' in feat:\n",
    "            parts = feat.split('>')\n",
    "        \n",
    "        for part in parts:\n",
    "            if part.replace('.','').replace(' ','').isdigit()==False:\n",
    "                feat_name = part.replace(' ','')\n",
    "        n = feat_list.index(feat_name)\n",
    "        weights[n].append(exp[1])\n",
    "    \n",
    "    weights = np.transpose(weights)\n",
    "    avg_weight = np.average(np.array(weights), axis = 0)\n",
    "    abs_weight = [abs(weight) for weight in avg_weight] \n",
    "        \n",
    "    #For average explanation    \n",
    "    bins = pd.cut(abs_weight, 10, retbins = True, duplicates = \"drop\")\n",
    "    quartile_min = bins[1][quartiles]\n",
    "    \n",
    "    rel_features = [i for i in range(len(abs_weight)) if abs_weight[i] > quartile_min]    \n",
    "    feat_names = [feat_list[i] for i in rel_features]\n",
    "    \n",
    "    rules = [exp[0] for exp in lime_exp]\n",
    "    \n",
    "    distribs = rule_breakdown(rules, rel_features, feat_names, train_data)\n",
    "    \n",
    "    return distribs\n",
    "\n",
    "\n",
    "def find_shap_val_distribution(instance, explainer, quartiles, percentage, global_exp, train_data, classification):\n",
    "    full_exp = [shap_explainer(instance, check_additivity = False).values for i in range(exp_iter)]\n",
    "    \n",
    "    if classification==True:\n",
    "        shap_exp = []\n",
    "        for each in full_exp:\n",
    "            single_exp = [feat[0] for feat in each]\n",
    "            shap_exp.append(single_exp)\n",
    "    else:\n",
    "        shap_exp = [exp.reshape(len(feat_list)) for exp in full_exp]\n",
    "\n",
    "    avg_shap_exp = np.average(shap_exp, axis = 0)\n",
    "    \n",
    "    abs_exp = [abs(exp) for exp in avg_shap_exp]\n",
    "\n",
    "    bins = pd.cut(abs_exp, 10, retbins = True, duplicates = \"drop\")\n",
    "    quartile_min = bins[1][quartiles]\n",
    "\n",
    "    rel_feats = [i for i in range(len(avg_shap_exp)) if abs_exp[i] > quartile_min]\n",
    "\n",
    "    distribs = []\n",
    "    \n",
    "\n",
    "    for loc in rel_feats:\n",
    "        orig_val = avg_shap_exp[loc]\n",
    "        bin_size = abs(orig_val * percentage)\n",
    "        min_val = orig_val-bin_size\n",
    "        max_val = orig_val+bin_size\n",
    "\n",
    "        feat_vals = []\n",
    "\n",
    "        for i in range(len(global_exp)):\n",
    "            if global_exp[i][loc] > min_val and global_exp[i][loc] < max_val:\n",
    "                feat_vals.append(train_data[i][loc])\n",
    "\n",
    "        max_feat = max(feat_vals)\n",
    "        min_feat = min(feat_vals)\n",
    "\n",
    "        distribs.append((loc, min_feat, max_feat))\n",
    "\n",
    "    return distribs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_feat_val_distribution(instance, explainer, explanation_method = \"lime\", cls = None, deciles = 9, percentage = 0.3, global_exp = None, train_data = None, classification = True):\n",
    "    \n",
    "    if deciles == 1:\n",
    "        bin_loc = -2\n",
    "    elif deciles == 2:\n",
    "        bin_loc = -3\n",
    "    elif deciles == 3:\n",
    "        bin_loc = -4\n",
    "    elif deciles == 4:\n",
    "        bin_loc = -5\n",
    "    elif deciles == 5:\n",
    "        bin_loc = -6\n",
    "    elif deciles == 6:\n",
    "        bin_loc = -7\n",
    "    elif deciles == 7:\n",
    "        bin_loc = -8\n",
    "    elif deciles == 8:\n",
    "        bin_loc = -9\n",
    "    else:\n",
    "        bin_loc = -10\n",
    "    \n",
    "    if explanation_method == \"shap\":\n",
    "        distribs = find_shap_val_distribution(instance, explainer, bin_loc, percentage, global_exp, train_data, classification)\n",
    "    \n",
    "    elif explanation_method == \"lime\":\n",
    "        distribs = find_lime_distribution(instance, explainer, cls, bin_loc, train_data, classification)\n",
    "        \n",
    "    return distribs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to project folder\n",
    "# please change to your own\n",
    "PATH = os.getcwd()\n",
    "\n",
    "dataset = \"income\"\n",
    "cls_method = \"xgboost\" \n",
    "classification = True\n",
    "\n",
    "if classification == True:\n",
    "    shap_deciles = 8\n",
    "    lime_deciles = 7\n",
    "    percentage = 0.5\n",
    "else:\n",
    "    shap_deciles = 9\n",
    "    lime_deciles = 9\n",
    "    percentage = 0.3\n",
    "\n",
    "random_state = 39\n",
    "exp_iter = 10\n",
    "random.seed(random_state)\n",
    "\n",
    "save_to = \"%s/%s/\" % (PATH, dataset)\n",
    "dataset_folder = \"%s/datasets/\" % (save_to)\n",
    "final_folder = \"%s/%s/\" % (save_to, cls_method)\n",
    "\n",
    "#Get datasets\n",
    "X_train = pd.read_csv(dataset_folder+dataset+\"_Xtrain.csv\", index_col=False, sep = \";\")\n",
    "test_x = pd.read_csv(final_folder+\"test_sample.csv\", index_col=False, sep = \";\").values\n",
    "\n",
    "results = pd.read_csv(os.path.join(final_folder,\"results.csv\"), index_col=False, sep = \";\")\n",
    "\n",
    "feat_list = [each.replace(' ','_') for each in X_train.columns]\n",
    "\n",
    "X = np.vstack((X_train.values, test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = joblib.load(save_to+cls_method+\"/cls.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lime_supporting = []\n",
    "lime_contrary = []\n",
    "\n",
    "if classification==True:\n",
    "    class_names=['Negative','Positive']# negative is 0, positive is 1, 0 is left, 1 is right\n",
    "    lime_explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, feature_names = feat_list, \n",
    "                                                            class_names=class_names, discretize_continuous=True)\n",
    "else:\n",
    "    class_names = ['Final Value']\n",
    "    lime_explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, feature_names = feat_list, \n",
    "                                                            class_names=class_names, discretize_continuous=True, mode = \"regression\")\n",
    "#index = 0\n",
    "for instance in test_x:\n",
    "    #index+=1\n",
    "    #print(index, \"of\", len(test_x))\n",
    "    distribution = find_feat_val_distribution(instance, lime_explainer, explanation_method = \"lime\", cls = cls, \n",
    "                                              train_data = X, classification = classification, deciles = lime_deciles)    \n",
    "    for i in range(len(distribution)):\n",
    "        each = list(distribution[i])\n",
    "        \n",
    "        all_val = [instance[each[0]] for instance in X]\n",
    "        feat_min = min(all_val)\n",
    "        feat_max = max(all_val)\n",
    "        interval = each[2]-each[1]\n",
    "                \n",
    "        if feat_min != each[1] or feat_max != each[2]:\n",
    "            new_min = each[1] - interval if each[1] - interval > feat_min else feat_min\n",
    "            new_max = each[2] + interval if each[2] + interval < feat_max else feat_max\n",
    "            \n",
    "            new_min = new_min if new_min != each[1] and new_min < each[1] else each[1] - interval\n",
    "            new_max = new_max if new_max != each[2] and new_max > each[2] else each[2] + interval\n",
    "                        \n",
    "            new_min = new_min if each[2] != feat_min else feat_min-100\n",
    "            new_max = new_max if each[1] != feat_max else feat_max+100\n",
    "            \n",
    "        else:\n",
    "            new_min = each[1] - interval\n",
    "            new_max = each[2] + interval\n",
    "        \n",
    "        each.append(new_min)\n",
    "        each.append(new_max)\n",
    "        \n",
    "        distribution[i] = tuple(each)\n",
    "        \n",
    "    if classification:\n",
    "        pred = cls.predict(instance.reshape(1, -1))\n",
    "        p1 = float(cls.predict_proba(instance.reshape(1, -1))[0][pred])\n",
    "    else:\n",
    "        p1 = cls.predict(instance.reshape(1, -1))[0]\n",
    "        \n",
    "    supp_diffs = []\n",
    "    for i in range(exp_iter):\n",
    "        alt_input = np.copy(instance)\n",
    "        for each in distribution:\n",
    "            alt_input[each[0]] = random.uniform(each[1], each[2])\n",
    "        if classification:\n",
    "            p2 = float(cls.predict_proba(alt_input.reshape(1, -1))[0][pred])\n",
    "        else:\n",
    "            p2 = cls.predict(instance.reshape(1, -1))[0]\n",
    "        diff = p1-p2\n",
    "        supp_diffs.append(diff)\n",
    "\n",
    "    supporting_mape = 1-calculate_mape(p1, supp_diffs)\n",
    "    lime_supporting.append(supporting_mape)\n",
    "    \n",
    "    cont_diffs = []\n",
    "    for i in range(exp_iter):\n",
    "        alt_input = np.copy(instance)\n",
    "        for each in distribution:\n",
    "            alt_input[each[0]] = generate_random_excluding(each[1], each[2], each[3], each[4])\n",
    "        if classification:\n",
    "            p2 = float(cls.predict_proba(alt_input.reshape(1, -1))[0][pred])\n",
    "        else:\n",
    "            p2 = cls.predict(instance.reshape(1, -1))[0]\n",
    "        diff = p1-p2\n",
    "        cont_diffs.append(diff)\n",
    "    contrary_mape = calculate_mape(p1, cont_diffs)\n",
    "    lime_contrary.append(contrary_mape)\n",
    "    \n",
    "\n",
    "results[\"LIME Supporting Fidelity\"] = lime_supporting\n",
    "results[\"LIME Contrary Fidelity\"] = lime_contrary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(lime_supporting))\n",
    "print(np.mean(lime_contrary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shap_supporting = []\n",
    "shap_contrary = []\n",
    "\n",
    "if cls_method == \"xgboost\":\n",
    "    shap_explainer = shap.Explainer(cls)\n",
    "else:\n",
    "    shap_explainer = shap.Explainer(cls, X, check_additivity = False)\n",
    "\n",
    "all_shap_val = []\n",
    "for i in range(exp_iter):\n",
    "    exp = shap_explainer(X, check_additivity = False).values\n",
    "    \n",
    "    if classification==True:\n",
    "        all_exp = []\n",
    "        for each in exp:\n",
    "            if len(each.shape) != 1:\n",
    "                single_exp = np.array([feat[0] for feat in each])\n",
    "                all_exp.append(single_exp)\n",
    "            else:\n",
    "                single_exp = each\n",
    "                all_exp.append(single_exp)\n",
    "    else:\n",
    "        all_exp = exp\n",
    "        \n",
    "    all_shap_val.append(all_exp)\n",
    "\n",
    "all_shap_exp = np.average(all_shap_val, axis = 0)\n",
    "\n",
    "\n",
    "for instance in test_x:\n",
    "    if cls_method == \"xgboost\":\n",
    "        instance = instance.reshape(1, -1)\n",
    "    distribution = find_feat_val_distribution(instance, shap_explainer, explanation_method = \"shap\", \n",
    "                                              global_exp = all_shap_exp, train_data = X, classification = classification, deciles = shap_deciles, percentage = percentage)\n",
    "    \n",
    "    for i in range(len(distribution)):\n",
    "        each = list(distribution[i])\n",
    "        \n",
    "        all_val = [instance[each[0]] for instance in X_train.values]\n",
    "        feat_min = min(all_val)\n",
    "        feat_max = max(all_val)\n",
    "        interval = each[2]-each[1]\n",
    "                \n",
    "        if feat_min != each[1] or feat_max != each[2]:\n",
    "            new_min = each[1] - interval if each[1] - interval > feat_min else feat_min\n",
    "            new_max = each[2] + interval if each[2] + interval < feat_max else feat_max\n",
    "            \n",
    "            new_min = new_min if new_min != each[1] and new_min < each[1] else each[1] - interval\n",
    "            new_max = new_max if new_max != each[2] and new_max > each[2] else each[2] + interval\n",
    "            \n",
    "        else:\n",
    "            new_min = each[1] - interval\n",
    "            new_max = each[2] + interval\n",
    "        \n",
    "        each.append(new_min)\n",
    "        each.append(new_max)\n",
    "        \n",
    "        distribution[i] = tuple(each)\n",
    "    \n",
    "    if classification:\n",
    "        pred = cls.predict(instance.reshape(1, -1))\n",
    "        p1 = float(cls.predict_proba(instance.reshape(1, -1))[0][pred])\n",
    "    else:\n",
    "        p1 = cls.predict(instance.reshape(1, -1))[0]\n",
    "    \n",
    "    \n",
    "    supp_diffs = []\n",
    "    for i in range(exp_iter):\n",
    "        alt_input = np.copy(instance.reshape(len(feat_list)))\n",
    "        for each in distribution:\n",
    "            alt_input[each[0]] = random.uniform(each[1], each[2])\n",
    "        if classification:\n",
    "            p2 = float(cls.predict_proba(alt_input.reshape(1, -1))[0][pred])\n",
    "        else:\n",
    "            p2 = cls.predict(instance.reshape(1, -1))[0]\n",
    "        diff = p1-p2\n",
    "        supp_diffs.append(diff)\n",
    "    supporting_mape = 1-calculate_mape(p1, supp_diffs)\n",
    "    shap_supporting.append(supporting_mape)\n",
    "    \n",
    "    cont_diffs = []\n",
    "    for i in range(exp_iter):\n",
    "        alt_input = np.copy(instance.reshape(len(feat_list)))\n",
    "        for each in distribution:\n",
    "            alt_input[each[0]] = generate_random_excluding(each[1], each[2], each[3], each[4])\n",
    "        if classification:\n",
    "            p2 = float(cls.predict_proba(alt_input.reshape(1, -1))[0][pred])\n",
    "        else:\n",
    "            p2 = cls.predict(instance.reshape(1, -1))[0]\n",
    "        diff = p1-p2\n",
    "        cont_diffs.append(diff)\n",
    "    contrary_mape = calculate_mape(p1, cont_diffs)\n",
    "    shap_contrary.append(contrary_mape)\n",
    "\n",
    "results[\"SHAP Supporting Fidelity\"] = shap_supporting\n",
    "results[\"SHAP Contrary Fidelity\"] = shap_contrary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(shap_supporting))\n",
    "print(np.mean(shap_contrary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_explainer.features = feat_list\n",
    "new_exp = shap_explainer(X)\n",
    "new_exp.feature_names = feat_list\n",
    "\n",
    "shap.plots.beeswarm(new_exp, max_display = len(feat_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(os.path.join(final_folder, \"results.csv\"), index = False, sep = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
